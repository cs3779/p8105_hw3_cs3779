---
title: "Homework 3"
author: "CJ Snyder"
date: "10/10/2019"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(viridis)
library(data.table)
library(patchwork)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# **Problem 1 - Uploading _Instacart_ Dataset:**
```{r, results='hide'}
library(p8105.datasets)
data ("instacart")

instacart
```

## **Dataset Description:**
The instacart dataset includes `r nrow(instacart)` observations, along with `r ncol(instacart)` variables including: `r names(instacart)`. 

Each item within an order is linked together using a particular _order_id_ and _product_id_, and includes information like when the order was made (*order_dow*, and *order_hour_of_day*), as well as where the item is located within the store (*`r names(instacart) [12:15]`*) and whether or not this item is being re-ordered (*reordered*). 
The amount of items ordered from each department is summarized in the below bar chart:
```{r, echo=FALSE}
instacart %>% 
  ggplot(aes(x=department)) + geom_bar(stat="count") +
    labs(x="Department where Item was Located", y="Amount Ordered") +
    scale_y_continuous(labels=scales::comma) +
    theme(axis.text.x = element_text(size=12,angle=90))
```

The largest number of orders occured on Sunday according to the below chart:
```{r, echo=FALSE}
instacart %>% 
  ggplot(aes(x=order_dow)) + geom_bar(stat="count") +
    labs(x="Day of the Week", y="Amount Ordered") +
    scale_y_continuous(labels=scales::comma) +
    scale_x_continuous(
      breaks = c(0,1,2,3,4,5,6),
      labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) +
    theme(axis.text.x = element_text(size=12))
```

```{r, echo=FALSE}
aisle_count = 
  instacart %>% 
  group_by(aisle) %>% 
  count() %>% 
  arrange(desc(n)) 
```

The total number of aisles collected by _Instacart_ is `r summarize(instacart, count=n_distinct(aisle_id))`. The aisle that had the most items ordered from it was **`r aisle_count[1,1]`** with *`r aisle_count[1,2]`* items bought, followed closely by **`r aisle_count[2,1]`** with *`r aisle_count[2,2]`* items bought. 

The aisles that had over 10,000 items bought from them are shown in the below plot (organized from the items that were bought the most to those that were bought the least):
```{r, echo=FALSE, eval=FALSE}
aisle_count %>% 
  filter(n>10000) %>% 
  ggplot(aes(x=reorder(aisle,desc(n)),y=n)) + 
    geom_col() +
    labs(x="Aisle where Item was Located", y="Amount Ordered") +
    scale_y_continuous(labels=scales::comma) +
    theme(axis.text.x = element_text(size=11,angle=90))
```

```{r, echo=FALSE}
aisle_count %>% 
  filter(n>10000) %>% 
  ggplot(aes(x=reorder(aisle,desc(n)),y=n)) + 
    geom_point() +
    labs(x="Aisle where Item was Located", y="Amount Ordered") +
    scale_y_continuous(labels=scales::comma) +
    theme(axis.text.x = element_text(size=11,angle=90))
```

The three most popular items bought from aisles labeled as _"baking ingredients"_, _"dog food care"_, and _"packaged vegetables fruits"_ are as follows:
```{r, echo=FALSE}
aisle_table = 
  instacart %>% 
  group_by(aisle, product_id, product_name) %>% 
  summarize(count_prod = n()) %>% 
  filter(aisle %in% c("baking ingredients",
                      "dog food care",
                      "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  top_n(3, count_prod) %>% 
  rename("Aisle" = aisle,
         "Amount Bought" = count_prod,
         "Prodcut ID" = product_id,
         "Product Name" = product_name)

kable(aisle_table)
```

For "baking ingredients", the three most bought items include _Pure Baking Soda_, _Light Brown Sugar_, and _Cane Sugar_. For "dog food care", they include _Snack Sticks Chicken & Rice Recipe Dog Treats_, _Small Dog Biscuits_, and _Organix Chicken & Brown Rice Recipe_. For "packaged vegetables fruits", they include _Organic Baby Spinach_, _Organic Raspberries_, and _Organic Blueberries_.

The mean hour of the day in which _Pink Lady Apples_ and _Coffee Ice Cream_ are ordered throughout the week are listed in the table below:
```{r, echo=FALSE}
mean_hour_df = 
  instacart %>% 
  group_by(product_name, order_dow) %>%
  mutate(
    mean_hour = mean(order_hour_of_day)) %>% 
  filter(product_name %in% c("Pink Lady Apples",
                             "Coffee Ice Cream")) %>% 
  select(product_name, order_dow, mean_hour) %>% 
  distinct(mean_hour) %>% 
  ungroup(order_dow) %>% 
  mutate(
    order_dow = as.character(order_dow),
    order_dow = factor(order_dow, c(0, 1, 2, 3, 4, 5, 6), 
                       labels=c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
  )
mean_hour_df = 
  mean_hour_df[order(mean_hour_df$order_dow), ] %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) 

kable(mean_hour_df)
```

According to the above table, it seems as thought "Pink Lady Apples" are purchased the earliest on Monday, and the latest on Wednesday. Meanwhile, "Coffee Ice Cream" is purchased the earliest on Friday, and the latest on Tuesday.

# **Problem 2 - Uploading _BRFSS_ Dataset:**
```{r, results='hide'}
library(p8105.datasets)
data("brfss_smart2010")

brfss_smart2010
```

## Data Cleaning for _brfss_smar2010_ dataset:
```{r, result='hide'}
names(brfss_smart2010)[] <- tolower(names(brfss_smart2010)[])

brfss_smart2010 %>% 
  filter(topic=="Overall Health") %>% 
  count(response)

brfss_smart2010 = 
  brfss_smart2010 %>% 
    filter(topic=="Overall Health",
           response %in% c("Excellent", "Fair", "Good", "Poor")
           ) %>%
  mutate(
    response = as.character(response),
    response = factor(response,levels=c("Poor", "Fair", "Good", "Excellent"))
  )
```

### Question 2a 
```{r}
brfss_2002 = 
  brfss_smart2010 %>% 
    filter(year=="2002") %>% 
    group_by(locationabbr) %>% 
    summarize(
      unique_location = n_distinct(locationdesc)
    ) %>% 
  filter(unique_location>=7)

kable(brfss_2002)
```
In 2002, the states that had 7 or more locations from which the government collected data include Connecticut, Florida, Massachusetts, North Carolina, New Jersey, and Pennsylvania

```{r}
brfss_2010 = 
  brfss_smart2010 %>% 
    filter(year=="2010") %>% 
    group_by(locationabbr) %>% 
    summarize(
      unique_location = n_distinct(locationdesc)
    ) %>% 
  filter(unique_location>=7)

kable(brfss_2010)
```
While in 2010, the stats that had 7 or more locations from which the government collected data include California, Colorodo, Florida, Massachusetts, Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio, Pennsylvania, South Carolina, Texas and Washington.

### Question 2b
```{r}
brfss_excel_df = 
  brfss_smart2010 %>% 
    filter(response=="Excellent") %>% 
  group_by(year, locationabbr) %>% 
  mutate(
    mean_data_value = mean(data_value)
  ) %>% 
  select(year, locationabbr, mean_data_value) %>% 
  distinct()
```

#### **Spaghetti-Plot**
```{r}
brfss_excel_df %>% 
  ggplot(aes(x=year, y=mean_data_value, group=locationabbr)) +
    geom_line(aes(color=locationabbr)) + 
  labs(
    x = "Year Data was Collected",
    y = "Mean Value of Data Value Across State Collection Locations"
  ) +
  theme(legend.key.width = unit(3,"cm"),
        legend.key.height = unit(0.25, "cm")) 
```

Overall the average "data value" seems to be decreasing over time (from 2002 - 2010) for all participating states. Over the span of this time frame, there seems to be large variability in the average "data value" for some states. However, there seems to be a trend over this range of time that indicates that West Viginia seems to have the lowest overall data value, while Washington D.C.s seems to have the greatest overall value.

### Question 2c
```{r, fig.show='hide'}
ny_plot_2006 = 
  brfss_smart2010 %>% 
  filter(year=="2006",
         locationabbr=="NY") %>%
  ggplot(aes(x=locationdesc, y=data_value, group=response, color=response)) +
  geom_point() +
  labs(
    title = "NY State 2006 BRFSS Data",
    x = "Data Collection Locations",
    y = "Data Value"
  )
ny_plot_2006

ny_plot_2010 = 
  brfss_smart2010 %>% 
  filter(year=="2010",
         locationabbr=="NY") %>%
  ggplot(aes(x=locationdesc, y=data_value, group=response, color=response)) +
  geom_point() +
  labs(
    title = "NY State 2010 BRFSS Data",
    x = "Data Collection Locations",
    y = "Data Value"
  ) +
  theme(axis.text.x = element_text(size=8,angle=45,vjust=0.45))
ny_plot_2010
```

#### 2006 & 2010 Distribution of Response-Type Data Value by NY State Location
```{r, echo=FALSE}
(ny_plot_2006) / (ny_plot_2010)
```

In 2006, the data value for any given "poor" response-type seemed very similar across locations. This can also be seen with any given "fair" response-type. However, when looking at "good" and "excellent" response-types, there seems to be some variability about how they are classified. For example, in all locations except for _New York County_ the "good" response type had a high data value than the "excellent" response-type. And the difference between these two responses also varried greatly, with _Kings County_ having the greatest difference, while for _Westchester County_ the values are practically the same.

Similarly to the 2006 dataset, the 2010 dataset still retains the same pattern of similar data value for "poor" and "fair" response-types across the various NY-state data collection locations. It is impotant to note that there are 3 additional locations that were established by 2010, including: _Bronx County_, _Erie County_, and _Monroe County_. In addition to _New York County_, now _Monroe County_, and _Westchester County_ have it so that their "excellent" response-type have a greater value than their "good" response-types. Also, instead of _Kings County_, it now appears that _Bronx County_ has the greatest difference between its "good" and "excellent" response-type data values.

# **Problem 3 - Uploading the _Accelerometer_ Dataset:**
```{r, results='hide'}
accel_df = read_csv(file="./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    day = as.character(day),
    dow = factor(day, c("Saturday", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), labels=c("Weekend", "Weekend", "Weekday", "Weekday", "Weekday", "Weekday", "Weekday"))
  )

accel_df[,4:1443] <- sapply(accel_df[, c(4:1443)], as.integer)
```
The _accel_df_ contains `r nrow(accel_df)` observations with `r ncol(accel_df)` variables. The majority of the varibales are activity counts for each minute of the day (starting at midnight, or 12 A.M.), or a total of 1,440 minutes per day. Each row represents a specific day of the week for the 7 total weeks data was colleceted on the subject (i.e. 35 totals rows).

## Total Activity for Each Day
```{r, results='hide'}
accel_df = 
  accel_df %>%
  mutate(
    total_activity = select(., 4:1443) %>% apply(1, sum, na.rm=TRUE)
  ) 
```
 
```{r}
accel_df_table =
  accel_df %>% 
  select(week, day_id, day, dow, total_activity) %>% 
  rename("Week" = week,
         "Day ID" = day_id,
         "Day of the Week" = day,
         "Weekday/Weekend" = dow,
         "Total Activity" = total_activity)

kable(accel_df_table)
```

Looking at the table of total activity over the 35 days data was collected on the subject, there was overall more activity at the beginning of observation, which diminished over time. The amount of activity seems to be fairly consistent throughout the week, but then the amount of activity from Friday to Monday drastically shifts from a lot to very little.

## 24-Hour Activity by Day Followed
```{r}
accel_df_plot = 
  accel_df %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_min",
    names_prefix = "activity_",
    values_to = "activity"
  ) %>% 
  select(-total_activity) %>% 
  mutate(
    activity_min = as.numeric(activity_min)
  )

accel_df_plot = 
  accel_df_plot %>% 
    group_by(activity_min = gl(ceiling(50400/60), 60, 50400)) %>% 
    mutate(hour_sum=sum(activity)) %>% 
    rename("activity_hour" = activity_min) %>% 
    ungroup() %>% 
    mutate(activity_hour = as.numeric(activity_hour))

activity_plot = 
  accel_df_plot %>% 
  ggplot(aes(x=activity_hour, y=hour_sum, group=day_id)) +
    geom_line(aes(color=day)) + 
  labs(
    title = "Activity per Day",
    x = "Activity every 24 Hours",
    y = "Activity Amount"
  ) +
  scale_x_continuous(
    breaks = seq(0, 840, 24)
  ) +
   theme(axis.text.x = element_text(angle=90))

activity_plot
```

The plot illustrates more clearly the patterns of activity that were seen in the above table. Where, at the initiation of observations there was overall a greater amount of activity than at the end of observation. A lot of the spike in activity occurred over the weekend, while activity was more or less steady throughout the weeekdays. Additionally, there are patterns of extremely small amounts of activity on a few Mondays and Sundays. This could be from forgetting to have worn the accelerometer, especially since the amount of activity is so small compared to all other times. 




